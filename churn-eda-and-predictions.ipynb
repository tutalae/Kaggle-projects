{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kopkritsaikhiao/churn-eda-and-predictions?scriptVersionId=136022588\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Welcome \n\nHey everyone, in this project, we'll be focusing on churn prediction. Our goal is to understand and predict churn using different methods. If you find this project interesting, please show your support by upvoting and sharing your feedback.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n    <img src=\"https://img.freepik.com/free-icon/loss_318-699915.jpg?size=626&ext=jpg\" alt=\"Loss Image\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-07T12:51:25.54933Z","iopub.execute_input":"2023-07-07T12:51:25.549831Z","iopub.status.idle":"2023-07-07T12:51:27.070827Z","shell.execute_reply.started":"2023-07-07T12:51:25.54979Z","shell.execute_reply":"2023-07-07T12:51:27.069642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # For creating plots\nimport matplotlib.ticker as mtick # For specifying the axes tick format \nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n\nsns.set(style = 'white')\n\n# Input data files are available in the \"../input/\" directory.\n\nimport os\n# print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:27.073521Z","iopub.execute_input":"2023-07-07T12:51:27.074284Z","iopub.status.idle":"2023-07-07T12:51:27.426498Z","shell.execute_reply.started":"2023-07-07T12:51:27.074239Z","shell.execute_reply":"2023-07-07T12:51:27.425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## preview the head of the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:27.430234Z","iopub.execute_input":"2023-07-07T12:51:27.431085Z","iopub.status.idle":"2023-07-07T12:51:27.516827Z","shell.execute_reply.started":"2023-07-07T12:51:27.431035Z","shell.execute_reply":"2023-07-07T12:51:27.515652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking all columns names\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:27.52029Z","iopub.execute_input":"2023-07-07T12:51:27.521106Z","iopub.status.idle":"2023-07-07T12:51:27.531158Z","shell.execute_reply.started":"2023-07-07T12:51:27.52106Z","shell.execute_reply":"2023-07-07T12:51:27.529537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the data types of all the columns\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:27.532658Z","iopub.execute_input":"2023-07-07T12:51:27.532995Z","iopub.status.idle":"2023-07-07T12:51:27.54916Z","shell.execute_reply.started":"2023-07-07T12:51:27.532964Z","shell.execute_reply":"2023-07-07T12:51:27.547772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## use pasdas profiling to make quick for understanding data","metadata":{}},{"cell_type":"code","source":"# Pandas profiling before data preprocessing\nprofile = ProfileReport(df, title='Pandas profiling before data preprcessing', minimal=True, progress_bar=False)\nprofile.to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:27.551332Z","iopub.execute_input":"2023-07-07T12:51:27.551749Z","iopub.status.idle":"2023-07-07T12:51:35.045849Z","shell.execute_reply.started":"2023-07-07T12:51:27.551715Z","shell.execute_reply":"2023-07-07T12:51:35.044541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## check missing colums and drop it","metadata":{}},{"cell_type":"code","source":"# Converting Total Charges to a numerical data type.\ndf.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:35.04771Z","iopub.execute_input":"2023-07-07T12:51:35.048419Z","iopub.status.idle":"2023-07-07T12:51:35.10649Z","shell.execute_reply.started":"2023-07-07T12:51:35.048379Z","shell.execute_reply":"2023-07-07T12:51:35.10508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop rows with missing values\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:35.109417Z","iopub.execute_input":"2023-07-07T12:51:35.109883Z","iopub.status.idle":"2023-07-07T12:51:35.156721Z","shell.execute_reply.started":"2023-07-07T12:51:35.10985Z","shell.execute_reply":"2023-07-07T12:51:35.155324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check columns is dropped\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:35.159118Z","iopub.execute_input":"2023-07-07T12:51:35.160905Z","iopub.status.idle":"2023-07-07T12:51:35.212349Z","shell.execute_reply.started":"2023-07-07T12:51:35.160851Z","shell.execute_reply":"2023-07-07T12:51:35.211098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:35.21692Z","iopub.execute_input":"2023-07-07T12:51:35.217336Z","iopub.status.idle":"2023-07-07T12:51:35.251817Z","shell.execute_reply.started":"2023-07-07T12:51:35.217304Z","shell.execute_reply":"2023-07-07T12:51:35.250624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print all unique value\ndef summary(df):\n    print(f\"Dataset has {df.shape[1]} features and {df.shape[0]} examples.\")\n    summary = pd.DataFrame(index=df.columns)\n    summary[\"Unique\"] = df.nunique().values\n    summary[\"Missing\"] = df.isnull().sum().values\n    summary[\"Duplicated\"] = df.duplicated().sum()\n    summary[\"Types\"] = df.dtypes\n    return summary\n\nsummary(df)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:35.253651Z","iopub.execute_input":"2023-07-07T12:51:35.254051Z","iopub.status.idle":"2023-07-07T12:51:35.351331Z","shell.execute_reply.started":"2023-07-07T12:51:35.254009Z","shell.execute_reply":"2023-07-07T12:51:35.349777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:35.353314Z","iopub.execute_input":"2023-07-07T12:51:35.353724Z","iopub.status.idle":"2023-07-07T12:51:35.362775Z","shell.execute_reply.started":"2023-07-07T12:51:35.353692Z","shell.execute_reply":"2023-07-07T12:51:35.361191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore relationships between each features and churn","metadata":{}},{"cell_type":"code","source":"selected_columns = ['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n       'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod']\n\nfor each_name in selected_columns:\n\n    each_count = df.groupby([each_name, 'Churn']).size().reset_index(name='Count')\n\n    fig = px.bar(each_count, x=f'{each_name}', y='Count', color='Churn', barmode='group')\n\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:35.364801Z","iopub.execute_input":"2023-07-07T12:51:35.365184Z","iopub.status.idle":"2023-07-07T12:51:37.054402Z","shell.execute_reply.started":"2023-07-07T12:51:35.365153Z","shell.execute_reply":"2023-07-07T12:51:37.05279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## what we see form this relationship \nBased on the plot, we can observe the following patterns in the churn relationship:\n\n* Senior citizens are more likely to churn.\n* Customers without partners are more likely to churn.\n* Customers without dependents are more likely to churn.\n* Customers who use fiber optic service are more likely to churn.\n* Customers without internet service are more likely to churn.\n* Customers without online security, online backup, device protection, and tech support are more likely to churn.\n* Customers who pay on a month-to-month basis are more likely to churn.\n* Customers who use paperless billing and pay with electronic checks are more likely to churn.\n\nThese factors indicate a higher probability of churn based on the plot.\n\nwith no machine learning we can see the differce between each features and churn.","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(df, x='MonthlyCharges', y='TotalCharges')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.056855Z","iopub.execute_input":"2023-07-07T12:51:37.057419Z","iopub.status.idle":"2023-07-07T12:51:37.150025Z","shell.execute_reply.started":"2023-07-07T12:51:37.057379Z","shell.execute_reply":"2023-07-07T12:51:37.148641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.box(df, x='Churn', y='tenure')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.15203Z","iopub.execute_input":"2023-07-07T12:51:37.152505Z","iopub.status.idle":"2023-07-07T12:51:37.269207Z","shell.execute_reply.started":"2023-07-07T12:51:37.152465Z","shell.execute_reply":"2023-07-07T12:51:37.267805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* this plot we can see that the distribution between who churn and tenure are different ","metadata":{}},{"cell_type":"code","source":"fig = px.box(df, x='Churn', y='MonthlyCharges')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.270947Z","iopub.execute_input":"2023-07-07T12:51:37.271319Z","iopub.status.idle":"2023-07-07T12:51:37.376421Z","shell.execute_reply.started":"2023-07-07T12:51:37.271288Z","shell.execute_reply":"2023-07-07T12:51:37.375132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* this plot we can see that the distribution between who churn and monthly charge are different ","metadata":{}},{"cell_type":"code","source":"fig = px.box(df, x='Churn', y='TotalCharges')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.378085Z","iopub.execute_input":"2023-07-07T12:51:37.378444Z","iopub.status.idle":"2023-07-07T12:51:37.48324Z","shell.execute_reply.started":"2023-07-07T12:51:37.378417Z","shell.execute_reply":"2023-07-07T12:51:37.48168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The plot shows that there is a difference in the distribution between customers who churn and their total charges. It suggests that customers with higher total charges have a lower chance of churning.\n\nIn other words, there seems to be a negative relationship between total charges and churn. Customers who have higher total charges are less likely to churn compared to those with lower total charges.","metadata":{}},{"cell_type":"markdown","source":"## Based on my initial analysis before using machine learning, here are my simple conclusions and assumptions:\n\n1. New users who do things online, like paying bills and receiving electronic statements, are more likely to stop using the service (churn). This is because they are already comfortable with online processes and may find it easy to switch to a different provider.\n\n2. Customers who don't use online security or protection features tend to choose basic services with lower prices. They prioritize saving money over having additional security measures.\n\nThese simple conclusions and assumptions provide insights into potential factors that might affect churn behavior based on customer characteristics and behaviors.","metadata":{}},{"cell_type":"markdown","source":"# Machine lelearning part","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.484786Z","iopub.execute_input":"2023-07-07T12:51:37.485131Z","iopub.status.idle":"2023-07-07T12:51:37.513526Z","shell.execute_reply.started":"2023-07-07T12:51:37.485102Z","shell.execute_reply":"2023-07-07T12:51:37.512076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.515229Z","iopub.execute_input":"2023-07-07T12:51:37.516468Z","iopub.status.idle":"2023-07-07T12:51:37.530215Z","shell.execute_reply.started":"2023-07-07T12:51:37.516411Z","shell.execute_reply":"2023-07-07T12:51:37.528718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# compare two one-hot encoding","metadata":{}},{"cell_type":"code","source":"df_no_id = df[['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n       'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']]\n\ndf_no_id.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.532495Z","iopub.execute_input":"2023-07-07T12:51:37.532979Z","iopub.status.idle":"2023-07-07T12:51:37.568319Z","shell.execute_reply.started":"2023-07-07T12:51:37.532928Z","shell.execute_reply":"2023-07-07T12:51:37.566747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_id.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.57072Z","iopub.execute_input":"2023-07-07T12:51:37.572133Z","iopub.status.idle":"2023-07-07T12:51:37.617801Z","shell.execute_reply.started":"2023-07-07T12:51:37.572087Z","shell.execute_reply":"2023-07-07T12:51:37.61656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## pd.get_dummies","metadata":{}},{"cell_type":"code","source":"# Perform one-hot encoding on selected columns\ndf_encoded_dummies = pd.get_dummies(df_no_id)\n\n# Display the encoded dataframe\ndf_encoded_dummies.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.619179Z","iopub.execute_input":"2023-07-07T12:51:37.620624Z","iopub.status.idle":"2023-07-07T12:51:37.677111Z","shell.execute_reply.started":"2023-07-07T12:51:37.620577Z","shell.execute_reply":"2023-07-07T12:51:37.676243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_object = df[['gender', 'Partner', 'Dependents',\n       'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod', 'Churn']]\n\ndf_object.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.6782Z","iopub.execute_input":"2023-07-07T12:51:37.679073Z","iopub.status.idle":"2023-07-07T12:51:37.704153Z","shell.execute_reply.started":"2023-07-07T12:51:37.679039Z","shell.execute_reply":"2023-07-07T12:51:37.702597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## OneHotEncoder with sklearn.preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Select the columns you want to encode\ncolumns_to_encode = df_object.columns\n\n# Create an instance of the OneHotEncoder class\nencoder = OneHotEncoder(sparse=False, drop='first')\n\n# Fit and transform the selected columns\nencoded_data = encoder.fit_transform(df[columns_to_encode])\n\n# Create a DataFrame with the encoded data\ndf_encoded = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns_to_encode))\n\n# Concatenate the encoded DataFrame with the original DataFrame\ndf_final = pd.concat([df.drop(columns=['customerID']+list(columns_to_encode)), df_encoded], axis=1)\n\n# Display the encoded DataFrame\ndf_final.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.705709Z","iopub.execute_input":"2023-07-07T12:51:37.70605Z","iopub.status.idle":"2023-07-07T12:51:37.882399Z","shell.execute_reply.started":"2023-07-07T12:51:37.706022Z","shell.execute_reply":"2023-07-07T12:51:37.880873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_encoded_dummies.columns), len(df_final.columns)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.884498Z","iopub.execute_input":"2023-07-07T12:51:37.88494Z","iopub.status.idle":"2023-07-07T12:51:37.892528Z","shell.execute_reply.started":"2023-07-07T12:51:37.884904Z","shell.execute_reply":"2023-07-07T12:51:37.891618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Using pd.get_dummies makes it straightforward to encode categorical variables, but it can result in an increase in the number of dimensions in the dataset. \n\n* scikit-learn's encoding techniques handle this issue by automatically removing redundant or repeated feature dimensions.","metadata":{}},{"cell_type":"code","source":"# Compare column sets\ncolumns_diff = set(df_encoded_dummies.columns) - set(df_final.columns)\n\n# Print the differing columns\nprint(\"Columns that are present in df_encoded but not in df_final:\")\nfor column in columns_diff:\n    print(column)\n\n# Repeat the comparison for columns present in df_final but not in df_encoded\ncolumns_diff = set(df_final.columns) - set(df_encoded.columns)\nprint(\"Columns that are present in df_final but not in df_encoded:\")\nfor column in columns_diff:\n    print(column)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.89375Z","iopub.execute_input":"2023-07-07T12:51:37.894135Z","iopub.status.idle":"2023-07-07T12:51:37.907797Z","shell.execute_reply.started":"2023-07-07T12:51:37.894104Z","shell.execute_reply":"2023-07-07T12:51:37.906766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Shapiro-Wilk test statistic is typically denoted as W. It has a range of values between 0 and 1, where a value close to 1 indicates a good fit to a normal distribution, and a value close to 0 suggests departure from normality.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import shapiro\n\n# Check for Gaussian distribution in each column\nfor column in df_encoded.columns:\n    data = df_encoded[column]\n    shapiro_stat, p_value = shapiro(data)\n    is_gaussian = p_value > 0.05  # Set significance level as 0.05\n\n    print(f\"Column '{column}':\")\n    print(f\"Shapiro-Wilk test statistic: {shapiro_stat}\")\n    print(f\"P-value: {p_value}\")\n    print(f\"Is Gaussian: {is_gaussian}\")\n    print(\"---------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.909741Z","iopub.execute_input":"2023-07-07T12:51:37.910079Z","iopub.status.idle":"2023-07-07T12:51:37.945594Z","shell.execute_reply.started":"2023-07-07T12:51:37.910049Z","shell.execute_reply":"2023-07-07T12:51:37.944391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_encoded_dummies.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.954017Z","iopub.execute_input":"2023-07-07T12:51:37.954777Z","iopub.status.idle":"2023-07-07T12:51:37.964045Z","shell.execute_reply.started":"2023-07-07T12:51:37.95473Z","shell.execute_reply":"2023-07-07T12:51:37.962794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train test split","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Define the features and target variable\nfeatures = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges',\n       'gender_Female', 'gender_Male', 'Partner_No', 'Partner_Yes',\n       'Dependents_No', 'Dependents_Yes', 'PhoneService_No',\n       'PhoneService_Yes', 'MultipleLines_No',\n       'MultipleLines_No phone service', 'MultipleLines_Yes',\n       'InternetService_DSL', 'InternetService_Fiber optic',\n       'InternetService_No', 'OnlineSecurity_No',\n       'OnlineSecurity_No internet service', 'OnlineSecurity_Yes',\n       'OnlineBackup_No', 'OnlineBackup_No internet service',\n       'OnlineBackup_Yes', 'DeviceProtection_No',\n       'DeviceProtection_No internet service', 'DeviceProtection_Yes',\n       'TechSupport_No', 'TechSupport_No internet service', 'TechSupport_Yes',\n       'StreamingTV_No', 'StreamingTV_No internet service', 'StreamingTV_Yes',\n       'StreamingMovies_No', 'StreamingMovies_No internet service',\n       'StreamingMovies_Yes', 'Contract_Month-to-month', 'Contract_One year',\n       'Contract_Two year', 'PaperlessBilling_No', 'PaperlessBilling_Yes',\n       'PaymentMethod_Bank transfer (automatic)',\n       'PaymentMethod_Credit card (automatic)',\n       'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check']\n\ntarget = ['Churn_Yes']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df_encoded_dummies[features], np.array(df_encoded_dummies[target]).ravel(), test_size=0.2, random_state=42)\nprint(\"total features :\", len(df_encoded_dummies[features].columns))\nprint(\"X \\n\", X_train.iloc[0], \"\\n y \\n\", y_train[0])","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:37.965782Z","iopub.execute_input":"2023-07-07T12:51:37.966174Z","iopub.status.idle":"2023-07-07T12:51:38.04272Z","shell.execute_reply.started":"2023-07-07T12:51:37.966143Z","shell.execute_reply":"2023-07-07T12:51:38.041529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.044261Z","iopub.execute_input":"2023-07-07T12:51:38.045525Z","iopub.status.idle":"2023-07-07T12:51:38.054507Z","shell.execute_reply.started":"2023-07-07T12:51:38.045478Z","shell.execute_reply":"2023-07-07T12:51:38.05299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.ravel()\ny_test = y_test.ravel()\n\ny_train","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.056477Z","iopub.execute_input":"2023-07-07T12:51:38.056864Z","iopub.status.idle":"2023-07-07T12:51:38.069349Z","shell.execute_reply.started":"2023-07-07T12:51:38.056834Z","shell.execute_reply":"2023-07-07T12:51:38.067802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression with pd.get_dummies","metadata":{}},{"cell_type":"code","source":"# Create the pipeline\npipeline = Pipeline([\n    ('scaler', MinMaxScaler()),      # Apply feature scaling using MinMaxScaler\n    ('classifier', LogisticRegression())    # Logistic Regression classifier\n])\n\n# Fit the pipeline on the training data\n\npipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.071764Z","iopub.execute_input":"2023-07-07T12:51:38.072193Z","iopub.status.idle":"2023-07-07T12:51:38.251595Z","shell.execute_reply.started":"2023-07-07T12:51:38.072162Z","shell.execute_reply":"2023-07-07T12:51:38.250358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = pipeline.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.253326Z","iopub.execute_input":"2023-07-07T12:51:38.254338Z","iopub.status.idle":"2023-07-07T12:51:38.280255Z","shell.execute_reply.started":"2023-07-07T12:51:38.254294Z","shell.execute_reply":"2023-07-07T12:51:38.278983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.282096Z","iopub.execute_input":"2023-07-07T12:51:38.283945Z","iopub.status.idle":"2023-07-07T12:51:38.297538Z","shell.execute_reply.started":"2023-07-07T12:51:38.283894Z","shell.execute_reply":"2023-07-07T12:51:38.295516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression with sklearn one-hot-encoder","metadata":{}},{"cell_type":"code","source":"df_final.isna().sum()\ndf_final.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.300208Z","iopub.execute_input":"2023-07-07T12:51:38.301288Z","iopub.status.idle":"2023-07-07T12:51:38.31715Z","shell.execute_reply.started":"2023-07-07T12:51:38.301224Z","shell.execute_reply":"2023-07-07T12:51:38.315517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.319535Z","iopub.execute_input":"2023-07-07T12:51:38.32061Z","iopub.status.idle":"2023-07-07T12:51:38.33675Z","shell.execute_reply.started":"2023-07-07T12:51:38.320519Z","shell.execute_reply":"2023-07-07T12:51:38.334779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the features and target variable\nfeatures = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges',\n       'gender_Male', 'Partner_Yes', 'Dependents_Yes', 'PhoneService_Yes',\n       'MultipleLines_No phone service', 'MultipleLines_Yes',\n       'InternetService_Fiber optic', 'InternetService_No',\n       'OnlineSecurity_No internet service', 'OnlineSecurity_Yes',\n       'OnlineBackup_No internet service', 'OnlineBackup_Yes',\n       'DeviceProtection_No internet service', 'DeviceProtection_Yes',\n       'TechSupport_No internet service', 'TechSupport_Yes',\n       'StreamingTV_No internet service', 'StreamingTV_Yes',\n       'StreamingMovies_No internet service', 'StreamingMovies_Yes',\n       'Contract_One year', 'Contract_Two year', 'PaperlessBilling_Yes',\n       'PaymentMethod_Credit card (automatic)',\n       'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check']\n\ntarget = ['Churn_Yes']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df_final[features], df_final[target], test_size=0.2, random_state=42)\nprint(\"total features :\", len(df_encoded_dummies[features].columns))\nprint(\"X \\n\", X_train.iloc[0], \"\\n y \\n\", y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.33974Z","iopub.execute_input":"2023-07-07T12:51:38.34073Z","iopub.status.idle":"2023-07-07T12:51:38.381635Z","shell.execute_reply.started":"2023-07-07T12:51:38.340672Z","shell.execute_reply":"2023-07-07T12:51:38.380125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.array(y_train).ravel()\ny_test = np.array(y_test).ravel()\n\ny_train","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.383993Z","iopub.execute_input":"2023-07-07T12:51:38.384996Z","iopub.status.idle":"2023-07-07T12:51:38.398281Z","shell.execute_reply.started":"2023-07-07T12:51:38.384939Z","shell.execute_reply":"2023-07-07T12:51:38.397261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the pipeline\npipeline = Pipeline([\n    ('scaler', MinMaxScaler()),      # Apply feature scaling using MinMaxScaler\n    ('classifier', LogisticRegression())    # Logistic Regression classifier\n])\n\n# Fit the pipeline on the training data\n\npipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.399786Z","iopub.execute_input":"2023-07-07T12:51:38.40035Z","iopub.status.idle":"2023-07-07T12:51:38.503317Z","shell.execute_reply.started":"2023-07-07T12:51:38.400319Z","shell.execute_reply":"2023-07-07T12:51:38.501748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = pipeline.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.505792Z","iopub.execute_input":"2023-07-07T12:51:38.50684Z","iopub.status.idle":"2023-07-07T12:51:38.529723Z","shell.execute_reply.started":"2023-07-07T12:51:38.506781Z","shell.execute_reply":"2023-07-07T12:51:38.527957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### results\n\n* form pandas pd.get_dummies get Accuracy: 0.7874911158493249\n* but form sklearn get Accuracy: 0.801423487544484\n\nbecause it have more repetitive dimensions \n\np(x) = 1 / (1 + e^(-z))\n\nthis is a formula for logistics regression \n\nhowever when we calculate logistic regression with many factors it will look like this \n\nz = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ\n\nit looklike when we calculate multiple linear regression.\nso, reduce dimensions is better.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Create the pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),      # Apply feature scaling using MinMaxScaler\n    ('classifier', LogisticRegression())    # Logistic Regression classifier\n])\n\n# Fit the pipeline on the training data\n\npipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.532071Z","iopub.execute_input":"2023-07-07T12:51:38.532999Z","iopub.status.idle":"2023-07-07T12:51:38.609468Z","shell.execute_reply.started":"2023-07-07T12:51:38.532946Z","shell.execute_reply":"2023-07-07T12:51:38.607914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = pipeline.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.612184Z","iopub.execute_input":"2023-07-07T12:51:38.61337Z","iopub.status.idle":"2023-07-07T12:51:38.637664Z","shell.execute_reply.started":"2023-07-07T12:51:38.613304Z","shell.execute_reply":"2023-07-07T12:51:38.63613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## change form minmax scale to standard scale\n\n* Min-max scaling, also known as normalization, is a data scaling technique that transforms the features to a specific range, typically between 0 and 1. It achieves this by subtracting the minimum value from each feature and then dividing it by the range (maximum value minus minimum value).\n\n* standard scaling, also known as z-score normalization, transforms the features to have a mean of 0 and a standard deviation of 1. It achieves this by subtracting the mean from each feature and then dividing it by the standard deviation.\n\nIn simpler terms, min-max scaling scales the features to a specific range, while standard scaling standardizes the features to have a mean of 0 and a standard deviation of 1.\n\nso, the result between Min-max scaling and standard scaling is not different significantly.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\n# Retrieve the logistic regression classifier from the pipeline\nlogreg = pipeline.named_steps['classifier']\n\n# Get the coefficients of the logistic regression model\ncoef = logreg.coef_[0]\n\n# Create a dataframe of variable names and corresponding coefficients\nweights_df = pd.DataFrame({'Variable': features, 'Weight': coef})\n\n# Sort the weights in descending order and select the top 10\ntop_weights = weights_df.sort_values('Weight', ascending=False).head(10)\n\n# Create the bar chart using Plotly\nfig = px.bar(top_weights, x='Variable', y='Weight', title='Top 10 Variable Weights')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.640159Z","iopub.execute_input":"2023-07-07T12:51:38.641132Z","iopub.status.idle":"2023-07-07T12:51:38.774391Z","shell.execute_reply.started":"2023-07-07T12:51:38.641073Z","shell.execute_reply":"2023-07-07T12:51:38.773157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\n# Retrieve the logistic regression classifier from the pipeline\nlogreg = pipeline.named_steps['classifier']\n\n# Get the coefficients of the logistic regression model\ncoef = logreg.coef_[0]\n\n# Create a dataframe of variable names and corresponding coefficients\nweights_df = pd.DataFrame({'Variable': features, 'Weight': coef})\n\n# Sort the weights in descending order and select the top 10\ntop_weights = weights_df.sort_values('Weight', ascending=True).head(10)\n\n# Create the bar chart using Plotly\nfig = px.bar(top_weights, x='Variable', y='Weight', title='Top 10 Variable Weights')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.775946Z","iopub.execute_input":"2023-07-07T12:51:38.776386Z","iopub.status.idle":"2023-07-07T12:51:38.853072Z","shell.execute_reply.started":"2023-07-07T12:51:38.776344Z","shell.execute_reply":"2023-07-07T12:51:38.851637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results indicate that certain features play a significant role in determining the outcome. \n\nThis suggests that when exploring the data, we can observe the influence of multiple features on the outcome variable.\n\nsuch as total charge, internet service fiber optic, tenure, mothly charge.","metadata":{}},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"markdown","source":"Random Forest is a machine learning algorithm that combines multiple decision trees to make predictions. It is called \"random\" because it creates each decision tree using a random subset of the training data and a random subset of the features.\n\nThink of it like a \"forest\" of decision trees, where each tree independently predicts the outcome, and the final prediction is made by averaging or voting across all the individual tree predictions.\n\nRandom Forest is known for its simplicity and ease of understanding. It can handle both classification and regression tasks and is effective in handling large datasets with many features. It is also robust to outliers and can provide insights into feature importance.\n\nIn simple terms, Random Forest is like a team of decision trees working together to make accurate predictions. Its randomness and combination of trees make it a powerful and easy-to-understand algorithm for machine learning tasks.\n\n<div style=\"text-align: center;\">\n    <img src=\"https://img.freepik.com/free-photo/aerial-view-vibrant-green-trees-forest_181624-49828.jpg?size=626&ext=jpg\" alt=\"forrest Image\">\n</div>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Create the pipeline with Random Forest Classifier\npipeline = Pipeline([\n    ('classifier', RandomForestClassifier())    # Random Forest Classifier\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:38.855119Z","iopub.execute_input":"2023-07-07T12:51:38.856266Z","iopub.status.idle":"2023-07-07T12:51:39.809855Z","shell.execute_reply.started":"2023-07-07T12:51:38.85623Z","shell.execute_reply":"2023-07-07T12:51:39.808313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = pipeline.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:39.812023Z","iopub.execute_input":"2023-07-07T12:51:39.81264Z","iopub.status.idle":"2023-07-07T12:51:39.927447Z","shell.execute_reply.started":"2023-07-07T12:51:39.812604Z","shell.execute_reply":"2023-07-07T12:51:39.925888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve the feature importances from the trained Random Forest model\nimportances = pipeline.named_steps['classifier'].feature_importances_\n\n# Create a dataframe of variable names and corresponding feature importances\nimportance_df = pd.DataFrame({'Variable': features, 'Importance': importances})\n\n# Sort the feature importances in descending order\nimportance_df = importance_df.sort_values('Importance', ascending=False)\n\n# Create the bar chart using Plotly\nfig = px.bar(importance_df, x='Variable', y='Importance', title='Feature Importance')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:39.929566Z","iopub.execute_input":"2023-07-07T12:51:39.930303Z","iopub.status.idle":"2023-07-07T12:51:40.025584Z","shell.execute_reply.started":"2023-07-07T12:51:39.930264Z","shell.execute_reply":"2023-07-07T12:51:40.02406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n# Define the pipeline\npipeline = Pipeline([\n    ('preprocessing', StandardScaler()),  # Preprocessing step\n    ('classifier', RandomForestClassifier(n_estimators=1000, \n                                          oob_score=True, \n                                          n_jobs=-1, \n                                          random_state=50, \n                                          max_features='sqrt', \n                                          max_leaf_nodes=30))  # Random Forest classifier\n])\n\n# Perform cross-validation\nscores = cross_val_score(estimator=pipeline, X=X_train, y=y_train, cv=5, scoring='roc_auc')\n\n# Print scores and their mean\nprint(\"score each round :\", scores)\nprint(\"Mean:\", scores.mean())","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:51:40.027088Z","iopub.execute_input":"2023-07-07T12:51:40.02745Z","iopub.status.idle":"2023-07-07T12:52:01.485214Z","shell.execute_reply.started":"2023-07-07T12:51:40.02742Z","shell.execute_reply":"2023-07-07T12:52:01.483844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a parameter grid for hyperparameter tuning with different values to be tested for 'n_estimators', 'max_depth', and 'max_features' hyperparameters\nparam_grid = [{'n_estimators': [100, 200, 300], 'max_depth': [None,2,3,10,20], 'max_features': ['sqrt',2,4,8,16,'log2', None]}]","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:52:01.487682Z","iopub.execute_input":"2023-07-07T12:52:01.488568Z","iopub.status.idle":"2023-07-07T12:52:01.4963Z","shell.execute_reply.started":"2023-07-07T12:52:01.488492Z","shell.execute_reply":"2023-07-07T12:52:01.494629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### tuning hyper-parameter","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Creating a random forest classifier object 'temp_rf' with a random state of 0 and parallel processing enabled\ntemp_rf=RandomForestClassifier(random_state=0,n_jobs=-1)\n# Creating a grid search object 'grid_search' using the 'GridSearchCV' function, with a random forest classifier as the estimator, a parameter grid, 'roc_auc' as the scoring metric, and 5-fold cross-validation with parallel processing\ngrid_search=GridSearchCV(estimator=temp_rf, param_grid=param_grid, scoring='roc_auc', cv=5, n_jobs=-1)\n# Performing grid search on the training data to find the best hyperparameters for the model\ngrid_search.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:52:01.498373Z","iopub.execute_input":"2023-07-07T12:52:01.499001Z","iopub.status.idle":"2023-07-07T12:57:04.368859Z","shell.execute_reply.started":"2023-07-07T12:52:01.49895Z","shell.execute_reply":"2023-07-07T12:57:04.367355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the best RMSE score found by Grid Search \ngrid_search.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:57:04.372019Z","iopub.execute_input":"2023-07-07T12:57:04.372766Z","iopub.status.idle":"2023-07-07T12:57:04.380308Z","shell.execute_reply.started":"2023-07-07T12:57:04.372723Z","shell.execute_reply":"2023-07-07T12:57:04.378851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieving the best parameter values found by the grid search\ngrid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:57:04.382142Z","iopub.execute_input":"2023-07-07T12:57:04.383347Z","iopub.status.idle":"2023-07-07T12:57:04.402666Z","shell.execute_reply.started":"2023-07-07T12:57:04.383309Z","shell.execute_reply":"2023-07-07T12:57:04.40138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Machine (SVC)","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Create the pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),      # Apply feature scaling using StandardScaler\n    ('classifier', SVC())              # Support Vector Machine classifier\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:57:04.40466Z","iopub.execute_input":"2023-07-07T12:57:04.405796Z","iopub.status.idle":"2023-07-07T12:57:05.782753Z","shell.execute_reply.started":"2023-07-07T12:57:04.405742Z","shell.execute_reply":"2023-07-07T12:57:05.781557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = pipeline.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:57:05.784321Z","iopub.execute_input":"2023-07-07T12:57:05.784752Z","iopub.status.idle":"2023-07-07T12:57:06.469926Z","shell.execute_reply.started":"2023-07-07T12:57:05.78472Z","shell.execute_reply":"2023-07-07T12:57:06.468446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost\n\n\nAdaBoost, short for Adaptive Boosting, is a machine learning algorithm that combines multiple weak or simple models (often decision trees) to create a strong predictive model. It is an ensemble method that aims to improve the performance of individual models by sequentially training them on different weighted versions of the dataset.\n\nThe key idea behind AdaBoost is to give more importance to the misclassified instances in each iteration, allowing subsequent models to focus on the difficult cases. In each iteration, the algorithm assigns higher weights to misclassified instances and lower weights to correctly classified instances. This way, subsequent models will pay more attention to the challenging observations.\n\nDuring the prediction phase, AdaBoost combines the predictions from all the weak models through a weighted voting or weighted averaging approach to make the final prediction.\n\nAdaBoost is known for its ability to handle complex datasets and achieve high accuracy. It is especially effective when combined with weak models, as it can effectively learn from their collective knowledge and generalize well to new, unseen data.\n\nIn simple terms, AdaBoost is a technique that combines multiple weak models to create a strong and accurate model. It learns from mistakes and gives more emphasis to challenging instances, resulting in improved performance.\n\n\n<div style=\"text-align: center;\">\n    <img src=\"https://static.packt-cdn.com/products/9781788295758/graphics/image_04_046-1.png\" alt=\"adaboost Image\">\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n# Create the pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),              # Apply feature scaling using StandardScaler\n    ('classifier', AdaBoostClassifier())       # AdaBoost Classifier\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:57:06.471978Z","iopub.execute_input":"2023-07-07T12:57:06.472694Z","iopub.status.idle":"2023-07-07T12:57:06.892238Z","shell.execute_reply.started":"2023-07-07T12:57:06.47265Z","shell.execute_reply":"2023-07-07T12:57:06.890853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = pipeline.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:57:06.894521Z","iopub.execute_input":"2023-07-07T12:57:06.895017Z","iopub.status.idle":"2023-07-07T12:57:06.956181Z","shell.execute_reply.started":"2023-07-07T12:57:06.894975Z","shell.execute_reply":"2023-07-07T12:57:06.954622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost\n\nXGBoost, short for eXtreme Gradient Boosting, is a powerful machine learning algorithm known for its high performance and efficiency. It belongs to the family of gradient boosting algorithms and is designed to optimize model performance through an iterative boosting process.\n\nXGBoost utilizes an ensemble of decision trees to make predictions. It builds each tree in a sequential manner, with each subsequent tree focusing on correcting the mistakes of the previous trees. The algorithm assigns higher weights to the misclassified instances, allowing subsequent trees to pay more attention to those instances.\n\nWhat sets XGBoost apart is its advanced regularization techniques, which help prevent overfitting and improve generalization. It incorporates both L1 and L2 regularization to control the complexity of the trees and includes a term to penalize complex models.\n\nXGBoost also features a unique approximation algorithm that speeds up the training process by considering only the most informative splits. It uses parallel processing capabilities and various optimization strategies to efficiently handle large datasets.\n\nThe algorithm offers flexibility with customizable parameters that allow fine-tuning for optimal performance. XGBoost is widely used across various domains and has achieved remarkable success in machine learning competitions.\n\nIn simple terms, XGBoost is an advanced gradient boosting algorithm that combines decision trees in an iterative process to make accurate predictions. It incorporates regularization techniques and optimization strategies to improve performance, making it a popular choice in machine learning tasks.\n\n<div style=\"text-align: center;\">\n    <img src=\"https://www.researchgate.net/publication/345327934/figure/fig3/AS:1022810793209856@1620868504478/Flow-chart-of-XGBoost.png\" alt=\"XGBoost Image\">\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Create the pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),              # Apply feature scaling using StandardScaler\n    ('classifier', XGBClassifier())            # XGBoost Classifier\n])\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:57:06.958197Z","iopub.execute_input":"2023-07-07T12:57:06.958674Z","iopub.status.idle":"2023-07-07T12:57:07.913392Z","shell.execute_reply.started":"2023-07-07T12:57:06.958623Z","shell.execute_reply":"2023-07-07T12:57:07.911969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on the test data\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = pipeline.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T12:57:07.915639Z","iopub.execute_input":"2023-07-07T12:57:07.916148Z","iopub.status.idle":"2023-07-07T12:57:07.945831Z","shell.execute_reply.started":"2023-07-07T12:57:07.916103Z","shell.execute_reply":"2023-07-07T12:57:07.944791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Differences between each model\n\nRandom Forest, AdaBoost, and XGBoost are all ensemble learning methods that combine multiple weak models to create a strong predictive model.\n\nHowever, there are some key differences between them:\n\n1. Random Forest:\n\n* It builds multiple decision trees independently and combines their predictions through voting or averaging.\n* Each tree is trained on a random subset of the training data and a random subset of features.\n* Random Forest is known for its simplicity, robustness against overfitting, and ability to handle high-dimensional data.\n* It performs well in a wide range of tasks and is relatively easy to interpret.\n\n2. AdaBoost (Adaptive Boosting):\n\n* It trains weak models sequentially, with each subsequent model focusing on correcting the mistakes of the previous models.\n* Instances that are misclassified by previous models are given higher weights, allowing subsequent models to pay more attention to them.\n* AdaBoost adapts to difficult cases by boosting the importance of misclassified instances.\n* It is effective in handling complex datasets and achieving high accuracy.\n\n3. XGBoost (eXtreme Gradient Boosting):\n\n* It is an optimized implementation of gradient boosting, designed for speed and performance.\n* XGBoost uses a combination of gradient descent optimization and regularization techniques.\n* It includes advanced regularization techniques, parallel processing capabilities, and optimization strategies.\n* XGBoost achieves high accuracy, handles large datasets efficiently, and is widely used in machine learning competitions.\n\n\nIn summary, Random Forest focuses on combining independently trained decision trees, AdaBoost adapts to challenging cases by boosting the importance of misclassified instances, and XGBoost is an optimized implementation of gradient boosting with advanced regularization and optimization techniques.","metadata":{}},{"cell_type":"markdown","source":"# Thank you for attention\n\nThank you for taking the time to read and engage with this information! If you found it helpful or enjoyable, please consider giving it an upvote. Positive feedback like this greatly motivates me to continue exploring new areas and sharing my knowledge.\n\nThis notebook provided me with many ideas for exploring data. I got inspiration from the techniques and insights shared in this Kaggle notebook: https://www.kaggle.com/code/arnabchaki/eda-on-data-science-salaries/notebook","metadata":{}}]}